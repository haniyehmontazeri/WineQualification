# -*- coding: utf-8 -*-
"""enhancedWinequalification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lLY8ZhcvA384ZidCeXjA01EYgii8biJO
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

# Load datasets
red_wine = pd.read_csv("winequality-red.csv", delimiter=';')
white_wine = pd.read_csv("winequality-white.csv", delimiter=';')

# Function to preprocess data
def preprocess_data(data):
    X = data.iloc[:, :-1]  # Features
    y = data.iloc[:, -1]   # Labels (quality)
    y = LabelEncoder().fit_transform(y)  # Encode quality labels as integers
    scaler = StandardScaler()
    X = scaler.fit_transform(X)  # Normalize features
    return X, y

# Preprocess red and white wine data
X_red, y_red = preprocess_data(red_wine)
X_white, y_white = preprocess_data(white_wine)

X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y_red, test_size=0.2, random_state=42)
X_train_white, X_test_white, y_train_white, y_test_white = train_test_split(X_white, y_white, test_size=0.2, random_state=42)

# Compute class weights to address class imbalance
class_weights_red = compute_class_weight('balanced', classes=np.unique(y_train_red), y=y_train_red)
class_weights_red = dict(enumerate(class_weights_red))

class_weights_white = compute_class_weight('balanced', classes=np.unique(y_train_white), y=y_train_white)
class_weights_white = dict(enumerate(class_weights_white))

# Function to build a neural network model
def build_model(input_dim, output_dim):
    model = Sequential([
        Dense(128, input_dim=input_dim, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation='relu'),
        BatchNormalization(),
        Dense(output_dim, activation='softmax')  # Output layer for classification
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Build models for red and white wine
model_red = build_model(X_train_red.shape[1], len(np.unique(y_red)))
model_white = build_model(X_train_white.shape[1], len(np.unique(y_white)))

# Define learning rate scheduler
def lr_scheduler(epoch, lr):
    if epoch > 10:
        return lr * 0.9  # Reduce LR by 10% after epoch 10
    return lr

scheduler = LearningRateScheduler(lr_scheduler)

# Train the red wine model
history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,
    callbacks=[scheduler],
    verbose=1
)

# Train the white wine model
history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white,
    callbacks=[scheduler],
    verbose=1
)

# Evaluate models
red_loss, red_acc = model_red.evaluate(X_test_red, y_test_red, verbose=0)
white_loss, white_acc = model_white.evaluate(X_test_white, y_test_white, verbose=0)

print(f"Red Wine Model Accuracy: {red_acc * 100:.2f}%")
print(f"White Wine Model Accuracy: {white_acc * 100:.2f}%")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Load datasets
red_wine = pd.read_csv("winequality-red.csv", delimiter=';')
white_wine = pd.read_csv("winequality-white.csv", delimiter=';')

# Function to preprocess data
def preprocess_data(data):
    X = data.iloc[:, :-1]  # Features
    y = data.iloc[:, -1]   # Labels (quality)
    scaler = StandardScaler()
    X = scaler.fit_transform(X)  # Normalize features
    return X, y

# Preprocess red and white wine data
X_red, y_red = preprocess_data(red_wine)
X_white, y_white = preprocess_data(white_wine)

# Apply SMOTE for oversampling minority classes
smote = SMOTE(random_state=42)
X_red, y_red = smote.fit_resample(X_red, y_red)
X_white, y_white = smote.fit_resample(X_white, y_white)

# Check the class distribution in red and white wine datasets
print("Red Wine Class Distribution:")
print(pd.Series(y_red).value_counts())

print("\nWhite Wine Class Distribution:")
print(pd.Series(y_white).value_counts())

from imblearn.over_sampling import SMOTE

# Apply SMOTE with n_neighbors=1 to avoid errors due to very few samples in minority class
smote = SMOTE(random_state=42, n_neighbors=1)  # Reducing n_neighbors to 1
X_red, y_red = smote.fit_resample(X_red, y_red)
X_white, y_white = smote.fit_resample(X_white, y_white)

pip install --upgrade imbalanced-learn

from imblearn.over_sampling import SMOTE

# Apply SMOTE with k_neighbors instead of n_neighbors
smote = SMOTE(random_state=42, k_neighbors=1)  # Using k_neighbors instead of n_neighbors
X_red, y_red = smote.fit_resample(X_red, y_red)
X_white, y_white = smote.fit_resample(X_white, y_white)

# Stratified train-test split to preserve class distribution
X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(
    X_red, y_red, test_size=0.2, random_state=42, stratify=y_red
)
X_train_white, X_test_white, y_train_white, y_test_white = train_test_split(
    X_white, y_white, test_size=0.2, random_state=42, stratify=y_white
)

# Compute class weights
class_weights_red = compute_class_weight('balanced', classes=np.unique(y_train_red), y=y_train_red)
class_weights_red = dict(enumerate(class_weights_red))

class_weights_white = compute_class_weight('balanced', classes=np.unique(y_train_white), y=y_train_white)
class_weights_white = dict(enumerate(class_weights_white))

# Function to build a simplified neural network model
def build_model(input_dim, output_dim):
    model = Sequential([
        Dense(64, input_dim=input_dim, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(32, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(output_dim, activation='softmax')  # Output layer for classification
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Build models for red and white wine
model_red = build_model(X_train_red.shape[1], len(np.unique(y_red)))
model_white = build_model(X_train_white.shape[1], len(np.unique(y_white)))

# Define early stopping and learning rate scheduler
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

def lr_scheduler(epoch, lr):
    if epoch > 10:
        return lr * 0.9  # Reduce LR by 10% after epoch 10
    return lr

scheduler = LearningRateScheduler(lr_scheduler)

# Train the red wine model
history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,
    callbacks=[early_stopping, scheduler],
    verbose=1
)

# Train the red wine model
history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,
    callbacks=[early_stopping, scheduler],
    verbose=1
)

# Train the white wine model
history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white,
    callbacks=[early_stopping, scheduler],
    verbose=1
)

# Train the red wine model
history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,
    callbacks=[early_stopping, scheduler],
    verbose=1
)

# Ensure y_train_red is properly encoded into integers
from sklearn.preprocessing import LabelEncoder

# Create a label encoder instance
encoder_red = LabelEncoder()

# Encode labels as integers
y_train_red = encoder_red.fit_transform(y_train_red)
y_test_red = encoder_red.transform(y_test_red)

# Check the unique values in y_train_red to verify encoding
print("Unique values in y_train_red after encoding:", np.unique(y_train_red))

# Check the shapes of X_train_red and y_train_red
print("Shape of X_train_red:", X_train_red.shape)
print("Shape of y_train_red:", y_train_red.shape)

from sklearn.utils import class_weight

# Calculate class weights for red wine dataset
class_weights_red = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train_red),
    y=y_train_red
)

# Convert class weights to dictionary format
class_weights_red_dict = dict(zip(np.unique(y_train_red), class_weights_red))
print("Class weights for red wine:", class_weights_red_dict)

history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red_dict,  # Use dictionary of class weights
    callbacks=[early_stopping, scheduler],
    verbose=1
)

model_red.add(Dense(6, activation='softmax'))  # 6 classes, softmax for multi-class classification

# Train the white wine model
history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white,
    callbacks=[early_stopping, scheduler],
    verbose=1
)

# Check the shape and unique values in y_train_white
print("Shape of y_train_white:", y_train_white.shape)
print("Unique values in y_train_white:", np.unique(y_train_white))

from sklearn.preprocessing import LabelEncoder

# Create and fit a LabelEncoder for the white wine labels
encoder_white = LabelEncoder()

# Encode y_train_white and y_test_white
y_train_white = encoder_white.fit_transform(y_train_white)
y_test_white = encoder_white.transform(y_test_white)

# Check unique values after encoding
print("Unique values in y_train_white after encoding:", np.unique(y_train_white))

# Check the shape of X_train_white and y_train_white
print("Shape of X_train_white:", X_train_white.shape)
print("Shape of y_train_white:", y_train_white.shape)

from sklearn.utils import class_weight

# Calculate class weights for white wine dataset
class_weights_white = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train_white),
    y=y_train_white
)

# Convert class weights to dictionary format
class_weights_white_dict = dict(zip(np.unique(y_train_white), class_weights_white))
print("Class weights for white wine:", class_weights_white_dict)

history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white_dict,  # Use dictionary of class weights
    callbacks=[early_stopping, scheduler],
    verbose=1
)

red_loss, red_acc = model_red.evaluate(X_test_red, y_test_red, verbose=0)
white_loss, white_acc = model_white.evaluate(X_test_white, y_test_white, verbose=0)

print(f"Red Wine Model Accuracy: {red_acc * 100:.2f}%")
print(f"White Wine Model Accuracy: {white_acc * 100:.2f}%")

from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight

# Label Encoding for white wine
encoder_white = LabelEncoder()
y_train_white = encoder_white.fit_transform(y_train_white)
y_test_white = encoder_white.transform(y_test_white)

# Check unique values after encoding
print("Unique values in y_train_white after encoding:", np.unique(y_train_white))

# Calculate class weights for white wine
class_weights_white = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train_white),
    y=y_train_white
)

# Convert class weights to dictionary format
class_weights_white_dict = dict(zip(np.unique(y_train_white), class_weights_white))
print("Class weights for white wine:", class_weights_white_dict)

from sklearn.preprocessing import StandardScaler

# Normalize the features for white wine
scaler_white = StandardScaler()
X_train_white = scaler_white.fit_transform(X_train_white)
X_test_white = scaler_white.transform(X_test_white)

# Check the first few rows of normalized data
print("First few rows of normalized X_train_white:", X_train_white[:5])

from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler

# Early stopping callback to stop training if validation loss does not improve
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Learning rate scheduler (optional, based on your implementation)
def scheduler(epoch, lr):
    if epoch % 10 == 0 and epoch > 0:
        lr = lr * 0.9  # Reduce learning rate by 10% every 10 epochs
    return lr

scheduler_callback = LearningRateScheduler(scheduler)

history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white_dict,  # Class weights to handle imbalanced classes
    callbacks=[early_stopping, scheduler_callback],
    verbose=1
)

from tensorflow.keras.optimizers import Adam

# Use Adam optimizer with learning rate adjustment
optimizer = Adam(learning_rate=0.001)

# Compile the model for white wine
model_white.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Check for missing values
print("Missing values in X_train_white:", np.sum(np.isnan(X_train_white)))
print("Missing values in y_train_white:", np.sum(np.isnan(y_train_white)))

# If any missing values, fill or remove them
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="mean")
X_train_white = imputer.fit_transform(X_train_white)
X_test_white = imputer.transform(X_test_white)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict on test set
y_pred_white = model_white.predict(X_test_white)
y_pred_white = np.argmax(y_pred_white, axis=1)

# Calculate accuracy and other metrics
print("Accuracy on white wine test set:", accuracy_score(y_test_white, y_pred_white))
print("Classification Report:\n", classification_report(y_test_white, y_pred_white))

print(f"Red Wine Model Accuracy: {red_accuracy * 100:.2f}%")
print(f"White Wine Model Accuracy: {white_accuracy * 100:.2f}%")

# Evaluate the red wine model on the test set
red_loss, red_accuracy = model_red.evaluate(X_test_red, y_test_red, verbose=0)

# Print the red wine model accuracy
print(f"Red Wine Model Accuracy: {red_accuracy * 100:.2f}%")

# Evaluate the white wine model on the test set
white_loss, white_accuracy = model_white.evaluate(X_test_white, y_test_white, verbose=0)

# Print the white wine model accuracy
print(f"White Wine Model Accuracy: {white_accuracy * 100:.2f}%")

print(f"Red Wine Model Accuracy: {red_accuracy * 100:.2f}%")
print(f"White Wine Model Accuracy: {white_accuracy * 100:.2f}%")

from tensorflow.keras.callbacks import ReduceLROnPlateau

# Reduce learning rate when the validation loss plateaus
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white,
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

from sklearn.utils import class_weight
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping

# Calculate class weights for red wine and white wine
class_weights_red = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train_red),
    y=y_train_red
)

class_weights_white = class_weight.compute_class_weight(
    'balanced',
    classes=np.unique(y_train_white),
    y=y_train_white
)

# Convert class_weights to a dictionary for each class
class_weights_red = {i: weight for i, weight in enumerate(class_weights_red)}
class_weights_white = {i: weight for i, weight in enumerate(class_weights_white)}

# Reduce learning rate when the validation loss plateaus
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the red wine model
history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,  # Pass class weights here
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

# Train the white wine model
history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white,  # Pass class weights here
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

# Accessing training and validation accuracy for red wine
train_accuracy_red = history_red.history['accuracy']
val_accuracy_red = history_red.history['val_accuracy']

# Accessing training and validation accuracy for white wine
train_accuracy_white = history_white.history['accuracy']
val_accuracy_white = history_white.history['val_accuracy']

# Print final accuracy for both red and white wine
print(f"Red Wine - Final training accuracy: {train_accuracy_red[-1]:.4f}")
print(f"Red Wine - Final validation accuracy: {val_accuracy_red[-1]:.4f}")

print(f"White Wine - Final training accuracy: {train_accuracy_white[-1]:.4f}")
print(f"White Wine - Final validation accuracy: {val_accuracy_white[-1]:.4f}")



from tensorflow.keras.layers import Dropout

# Modify model architecture by adding Dropout and increasing neurons
model_white = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_white.shape[1],)),
    Dropout(0.2),  # Add dropout layer to prevent overfitting
    Dense(64, activation='relu'),
    Dropout(0.2),  # Dropout for second hidden layer
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')  # Adjust based on your output classes
])

model_red = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_red.shape[1],)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')  # Adjust output layer for red wine classes
])

model_white.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_red.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])



# Print final accuracy for both red and white wine
print(f"Red Wine - Final training accuracy: {train_accuracy_red[-1]:.4f}")
print(f"Red Wine - Final validation accuracy: {val_accuracy_red[-1]:.4f}")

print(f"White Wine - Final training accuracy: {train_accuracy_white[-1]:.4f}")
print(f"White Wine - Final validation accuracy: {val_accuracy_white[-1]:.4f}")

# Train the red wine model with Dropout layers
history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,  # Splitting data into training and validation sets
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,
    callbacks=[early_stopping, lr_scheduler],  # Add your callbacks here
    verbose=1
)

# Train the white wine model with Dropout layers
history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,  # Splitting data into training and validation sets
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white,
    callbacks=[early_stopping, lr_scheduler],  # Add your callbacks here
    verbose=1
)

# For red wine model
train_accuracy_red = history_red.history['accuracy']  # Training accuracy for red wine
val_accuracy_red = history_red.history['val_accuracy']  # Validation accuracy for red wine

# For white wine model
train_accuracy_white = history_white.history['accuracy']  # Training accuracy for white wine
val_accuracy_white = history_white.history['val_accuracy']  # Validation accuracy for white wine

# Print final accuracy for both red and white wines
print(f"Red Wine - Final training accuracy: {train_accuracy_red[-1]:.4f}")
print(f"Red Wine - Final validation accuracy: {val_accuracy_red[-1]:.4f}")

print(f"White Wine - Final training accuracy: {train_accuracy_white[-1]:.4f}")
print(f"White Wine - Final validation accuracy: {val_accuracy_white[-1]:.4f}")

# Red Wine Model without Dropout and any new changes
model_red = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_red.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')  # Adjust output layer for red wine classes
])

# White Wine Model without Dropout and any new changes
model_white = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_white.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(10, activation='softmax')  # Adjust output layer for white wine classes
])

# Compile the models
model_red.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model_white.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the red wine model without Dropout
history_red = model_red.fit(
    X_train_red, y_train_red,
    validation_split=0.2,  # Splitting data into training and validation sets
    epochs=50,
    batch_size=32,
    class_weight=class_weights_red,  # If you used class weights before
    callbacks=[early_stopping, lr_scheduler],  # Add your callbacks if you used them before
    verbose=1
)

# Train the white wine model without Dropout
history_white = model_white.fit(
    X_train_white, y_train_white,
    validation_split=0.2,  # Splitting data into training and validation sets
    epochs=50,
    batch_size=32,
    class_weight=class_weights_white,  # If you used class weights before
    callbacks=[early_stopping, lr_scheduler],  # Add your callbacks if you used them before
    verbose=1
)

# Evaluate the models and print the final accuracies
train_accuracy_red = history_red.history['accuracy'][-1]
val_accuracy_red = history_red.history['val_accuracy'][-1]
train_accuracy_white = history_white.history['accuracy'][-1]
val_accuracy_white = history_white.history['val_accuracy'][-1]

print(f"Red Wine - Final training accuracy: {train_accuracy_red:.4f}")
print(f"Red Wine - Final validation accuracy: {val_accuracy_red:.4f}")
print(f"White Wine - Final training accuracy: {train_accuracy_white:.4f}")
print(f"White Wine - Final validation accuracy: {val_accuracy_white:.4f}")